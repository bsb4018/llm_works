{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kbI6R5VbO65",
        "outputId": "58ce7063-7648-453c-979a-a10c3971776e"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q langchain==0.0.152"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6_crg9aaygO",
        "outputId": "7f060a4a-b327-41a5-ae00-759fb08516b4"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q pyllamacpp==1.0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN7-b5i4dXie",
        "outputId": "8c0066ae-fcde-442d-8060-49da4beab9dc"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"D:/work4/activeloop_LLM/.env\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uelopmxka9Ok"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import GPT4All\n",
        "from langchain.callbacks.base import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf_4IWlTbV28"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M8WzOr2br0s"
      },
      "outputs": [],
      "source": [
        "local_path = './models/gpt4all-lora-quantized-ggml.bin'  # replace with your desired local file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doxYYzl2bbzf",
        "outputId": "68e12e6d-cf8d-4670-a4f8-a46500b28269"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "local_path = './models/gpt4all-lora-quantized-ggml.bin'\n",
        "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Example model. Check https://github.com/nomic-ai/pyllamacpp for the latest models.\n",
        "url = 'https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin'\n",
        "# url = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n",
        "\n",
        "# send a GET request to the URL to download the file. Stream since it's large\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "# open the file in binary mode and write the contents of the response to it in chunks\n",
        "# This is a large file, so be prepared to wait.\n",
        "with open(local_path, 'wb') as f:\n",
        "    for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
        "        if chunk:\n",
        "            f.write(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwkToQRwo2rV"
      },
      "source": [
        "https://github.com/ggerganov/llama.cpp#using-gpt4all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElQJOhoWlMLZ",
        "outputId": "1666a53b-7a01-468b-c788-4ccdd0a6f4cf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "!cd llama.cpp && git checkout 2b26469"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H23sS2bmSvI",
        "outputId": "52a43a78-3c31-432d-9692-5a4ec070c074"
      },
      "outputs": [],
      "source": [
        "!python3 llama.cpp/convert.py ./models/gpt4all-lora-quantized-ggml.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkTpiTbobtOV"
      },
      "outputs": [],
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-nZRMRUd2OJ"
      },
      "outputs": [],
      "source": [
        "# Verbose is required to pass to the callback manager\n",
        "llm = GPT4All(model=\"./models/ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6VRxCcoioOQ"
      },
      "outputs": [],
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoHI4dUsrQd1"
      },
      "source": [
        "# Documentation Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GXwzxf3n2L_"
      },
      "outputs": [],
      "source": [
        "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "v8yIT-pDn5Yt",
        "outputId": "3596d4ad-9ca4-4429-deaa-126659199184"
      },
      "outputs": [],
      "source": [
        "llm_chain.run(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "561e16I_q7-I"
      },
      "source": [
        "# Poetic Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-AEqJ8TBcKD"
      },
      "outputs": [],
      "source": [
        "question = \"Write a poem about friendship that rhymes.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "vxGIzeR7Bc7O",
        "outputId": "e75789ca-650e-4a25-f100-156202ee4c6f"
      },
      "outputs": [],
      "source": [
        "llm_chain.run(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI8ADce7rJyD"
      },
      "source": [
        "# Mother's day Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CWDAKTKJm8n"
      },
      "outputs": [],
      "source": [
        "question = \"Write a social media post to celebrate mother's day.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "tPIl8hhYJneV",
        "outputId": "3b3398b6-8da7-40ef-c2ff-5868cf915fc8"
      },
      "outputs": [],
      "source": [
        "llm_chain.run(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBGcLlz-qnyK"
      },
      "source": [
        "# Explain Rain Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp6qtHUnvS0W"
      },
      "outputs": [],
      "source": [
        "question = \"What happens when it rains somewhere?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "ZVtsVANR52-A",
        "outputId": "7d444386-931a-4f84-ae42-d8c4c8aa3eec"
      },
      "outputs": [],
      "source": [
        "llm_chain.run(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O8NWPP2qqHm"
      },
      "source": [
        "# Rain Example, but one sentence and funny."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4oD6qpQqtCK"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's answer in two sentence while being funny.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBJ4q3wPq-U-"
      },
      "outputs": [],
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "_WKJCcqOrBM0",
        "outputId": "d9f4dc73-216b-49b5-dbf9-60becfe2d15e"
      },
      "outputs": [],
      "source": [
        "llm_chain.run(question)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
